Agent0: Unleashing Self-Evolving Agents from Zero Data
via Tool-Integrated Reasoning
Peng Xia 1 Kaide Zeng 1 Jiaqi Liu 1 Can Qin 2 Fang Wu3 Yiyang Zhou 1 Caiming Xiong 2 Huaxiu Yao1
Abstract
Large Language Model (LLM) Agents, often
trained with Reinforcement Learning (RL), are
constrained by a dependency on human-curated
data, limiting scalability and tethering AI to
human knowledge. Existing self-evolution
frameworks offer an alternative but are typically
restricted by the model’s inherent capabilities and
single-round interactions, hindering the devel-
opment of complex curricula involving tool use
or dynamic reasoning. We introduce Agent0,
a fully autonomous framework that evolves
high-performing agents without external data
through multi-step co-evolution and seamless
tool integration. Agent0 establishes a symbiotic
competition between two agents initialized from
the same base LLM: a curriculum agent that pro-
poses increasingly challenging frontier tasks, and
an executor agent that learns to solve them. We
integrate external tools to enhance the executor’s
problem-solving capacity; this improvement, in
turn, pressures the curriculum agent to construct
more complex, tool-aware tasks. Through
this iterative process, Agent0 establishes a
self-reinforcing cycle that continuously produces
high-quality curricula. Empirically, Agent0
substantially boosts reasoning capabilities,
improving the Qwen3-8B-Base model by 18%
on mathematical reasoning and 24% on general
reasoning benchmarks. Code is available at
https://github.com/aiming-lab/Agent0.
1. Introduction
Large Language Model (LLM) Agents have shown re-
markable capabilities in tackling complex, long-horizon
problems (Qiu et al., 2025b;a; Jin et al., 2025; Yu et al.,
2025a; Tang et al., 2025; Zhai et al., 2025) that require
1UNC-Chapel Hill 2Salesforce Research 3Stanford University.
Correspondence to: Peng Xia <pxia@cs.unc.edu>, Huaxiu Yao
<huaxiu@cs.unc.edu>.
Preliminary work.
extensive interaction with an environment, such as deep
research (OpenAI, 2025; Google, 2024; Team et al., 2025)
and agentic coding (Jimenez et al., 2023; Anthropic, 2025;
Wang et al., 2024a). To optimize these complex, multi-
step interactions and move beyond hard-coded workflows,
Reinforcement Learning (RL) has emerged as a principal
training paradigm (Ouyang et al., 2022; Shao et al., 2024;
Tu et al., 2025), achieving significant progress on com-
plex reasoning tasks. However, the efficacy of these meth-
ods, whether Reinforcement Learning from Human Feed-
back (RLHF) or Reinforcement Learning from Verifiable
Rewards (RLVR), relies heavily on massive, high-quality,
human-curated datasets (Zhang et al., 2025c). This depen-
dency not only creates a severe scalability bottleneck (Yue
et al., 2025), which is time-consuming, labor-intensive, and
costly, but also fundamentally tethers the potential of AI to
the limits of human knowledge and annotation speed.
To break free from this reliance on human data, self-
evolution frameworks have emerged as a promising alterna-
tive (Zhao et al., 2025; Liu et al., 2025a; Huang et al., 2025;
Wang et al., 2025d), offering a scalable pathway by en-
abling models to autonomously generate their own training
data. Yet, despite their potential, existing self-play or self-
challenging approaches face severe constraints. First, their
capabilities are capped by the model’s inherent knowledge
and reasoning abilities (Fang et al., 2025; Cheng et al., 2024;
Zhou et al., 2025a), causing the generated tasks to rarely
surpass the model’s current complexity (Zhou et al., 2025b),
leading to learning stagnation. Second, these frameworks
typically operate only in single-round interactions (Li et al.,
2025c), failing to capture the dynamic, context-dependent
nature of real-world problems. This dual limitation not only
restricts the complexity of the self-generated curriculum but,
more critically, hinders the model from mastering essential
skills that require complex tool use or multi-st
