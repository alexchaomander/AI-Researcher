{
    "messages": [
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"directory\": \"/workplace/agent0\"}",
                        "name": "gen_code_tree_structure"
                    },
                    "id": "call_QmIBarGk515wzSJaVjqLhRY5",
                    "type": "function"
                },
                {
                    "index": 1,
                    "function": {
                        "arguments": "{\"directory\": \"/workplace/llm_based_agents\"}",
                        "name": "gen_code_tree_structure"
                    },
                    "id": "call_0k70fyCUC9Q7zKR1FldogI1H",
                    "type": "function"
                },
                {
                    "index": 2,
                    "function": {
                        "arguments": "{\"directory\": \"/workplace/survey_on_llm_based_multi_agent_systems\"}",
                        "name": "gen_code_tree_structure"
                    },
                    "id": "call_WJVKnvZus0Rhwegzsjb9bPPb",
                    "type": "function"
                },
                {
                    "index": 3,
                    "function": {
                        "arguments": "{\"directory\": \"/workplace/evaluating_integration_LLM_agents\"}",
                        "name": "gen_code_tree_structure"
                    },
                    "id": "call_EQ99DRmHhtvl1xvMOpxfPp4d",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_QmIBarGk515wzSJaVjqLhRY5",
            "name": "gen_code_tree_structure",
            "content": "[The output of the tool `gen_code_tree_structure` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\n/workplace/agent0  [error opening dir]\n\n0 directories, 0 files\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_0k70fyCUC9Q7zKR1FldogI1H",
            "name": "gen_code_tree_structure",
            "content": "[The output of the tool `gen_code_tree_structure` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\n/workplace/llm_based_agents  [error opening dir]\n\n0 directories, 0 files\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_WJVKnvZus0Rhwegzsjb9bPPb",
            "name": "gen_code_tree_structure",
            "content": "[The output of the tool `gen_code_tree_structure` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\n/workplace/survey_on_llm_based_multi_agent_systems  [error opening dir]\n\n0 directories, 0 files\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_EQ99DRmHhtvl1xvMOpxfPp4d",
            "name": "gen_code_tree_structure",
            "content": "[The output of the tool `gen_code_tree_structure` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\n/workplace/evaluating_integration_LLM_agents  [error opening dir]\n\n0 directories, 0 files\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"file_path\": \"/workplace/agent0/README.md\"}",
                        "name": "read_file"
                    },
                    "id": "call_fLxbhXqHd5TzoutDTuRXx5Cu",
                    "type": "function"
                },
                {
                    "index": 1,
                    "function": {
                        "arguments": "{\"file_path\": \"/workplace/llm_based_agents/README.md\"}",
                        "name": "read_file"
                    },
                    "id": "call_FjCdLp7kuSjMdOTm2wfljIMi",
                    "type": "function"
                },
                {
                    "index": 2,
                    "function": {
                        "arguments": "{\"file_path\": \"/workplace/survey_on_llm_based_multi_agent_systems/README.md\"}",
                        "name": "read_file"
                    },
                    "id": "call_ixQc4Li8Imlil8GTWMeDdsFf",
                    "type": "function"
                },
                {
                    "index": 3,
                    "function": {
                        "arguments": "{\"file_path\": \"/workplace/evaluating_integration_LLM_agents/README.md\"}",
                        "name": "read_file"
                    },
                    "id": "call_wwZCht0cA2Wc1BL5vmlDJeY6",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_fLxbhXqHd5TzoutDTuRXx5Cu",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/agent0/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_FjCdLp7kuSjMdOTm2wfljIMi",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/llm_based_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_ixQc4Li8Imlil8GTWMeDdsFf",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/survey_on_llm_based_multi_agent_systems/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "role": "tool",
            "tool_call_id": "call_wwZCht0cA2Wc1BL5vmlDJeY6",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 0 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_78Tdwjg5etJqpFyAIHvzE3Qt",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_78Tdwjg5etJqpFyAIHvzE3Qt",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 1 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_NyMLqJyv2m9nBM39lYJmzYgc",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_NyMLqJyv2m9nBM39lYJmzYgc",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 2 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_cjdi53edlWwRxumFImBValtw",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_cjdi53edlWwRxumFImBValtw",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 3 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_wnNQUr7iEKbLXkxWuj7cNuAm",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_wnNQUr7iEKbLXkxWuj7cNuAm",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 4 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_xaIbUK9k2MTXJYby3wEsAOT2",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_xaIbUK9k2MTXJYby3wEsAOT2",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 5 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"page_idx\":1}",
                        "name": "terminal_page_to"
                    },
                    "id": "call_znBHpKVl8rmDlFHohAPCp0nU",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_znBHpKVl8rmDlFHohAPCp0nU",
            "name": "terminal_page_to",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nYou previously visited this page of terminal 6 seconds ago.\nTerminal viewport position: Showing page 1 of 1.\n==============================================\ncat: /workplace/evaluating_integration_LLM_agents/README.md: No such file or directory\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"directory\":\"/workplace\"}",
                        "name": "gen_code_tree_structure"
                    },
                    "id": "call_rkYeAMfluV5DsLv8zHjboheM",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_rkYeAMfluV5DsLv8zHjboheM",
            "name": "gen_code_tree_structure",
            "content": "[The output of the tool `gen_code_tree_structure` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\n/workplace\n├── papers\n│   ├── agent0.pdf\n│   ├── llm_agents_tool_learning_survey.pdf\n│   ├── llm_multi_agent_survey.pdf\n│   └── reasoning_action_integration.pdf\n└── supervisord.pid\n\n2 directories, 5 files\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"file_path\":\"/workplace/papers/agent0.pdf\"}",
                        "name": "read_file"
                    },
                    "id": "call_4KQdrUQKTHClUkCBGKlNsIWh",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_4KQdrUQKTHClUkCBGKlNsIWh",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\nAgent0: Unleashing Self-Evolving Agents from Zero Data\nvia Tool-Integrated Reasoning\nPeng Xia 1 Kaide Zeng 1 Jiaqi Liu 1 Can Qin 2 Fang Wu3 Yiyang Zhou 1 Caiming Xiong 2 Huaxiu Yao1\nAbstract\nLarge Language Model (LLM) Agents, often\ntrained with Reinforcement Learning (RL), are\nconstrained by a dependency on human-curated\ndata, limiting scalability and tethering AI to\nhuman knowledge. Existing self-evolution\nframeworks offer an alternative but are typically\nrestricted by the model’s inherent capabilities and\nsingle-round interactions, hindering the devel-\nopment of complex curricula involving tool use\nor dynamic reasoning. We introduce Agent0,\na fully autonomous framework that evolves\nhigh-performing agents without external data\nthrough multi-step co-evolution and seamless\ntool integration. Agent0 establishes a symbiotic\ncompetition between two agents initialized from\nthe same base LLM: a curriculum agent that pro-\nposes increasingly challenging frontier tasks, and\nan executor agent that learns to solve them. We\nintegrate external tools to enhance the executor’s\nproblem-solving capacity; this improvement, in\nturn, pressures the curriculum agent to construct\nmore complex, tool-aware tasks. Through\nthis iterative process, Agent0 establishes a\nself-reinforcing cycle that continuously produces\nhigh-quality curricula. Empirically, Agent0\nsubstantially boosts reasoning capabilities,\nimproving the Qwen3-8B-Base model by 18%\non mathematical reasoning and 24% on general\nreasoning benchmarks. Code is available at\nhttps://github.com/aiming-lab/Agent0.\n1. Introduction\nLarge Language Model (LLM) Agents have shown re-\nmarkable capabilities in tackling complex, long-horizon\nproblems (Qiu et al., 2025b;a; Jin et al., 2025; Yu et al.,\n2025a; Tang et al., 2025; Zhai et al., 2025) that require\n1UNC-Chapel Hill 2Salesforce Research 3Stanford University.\nCorrespondence to: Peng Xia <pxia@cs.unc.edu>, Huaxiu Yao\n<huaxiu@cs.unc.edu>.\nPreliminary work.\nextensive interaction with an environment, such as deep\nresearch (OpenAI, 2025; Google, 2024; Team et al., 2025)\nand agentic coding (Jimenez et al., 2023; Anthropic, 2025;\nWang et al., 2024a). To optimize these complex, multi-\nstep interactions and move beyond hard-coded workflows,\nReinforcement Learning (RL) has emerged as a principal\ntraining paradigm (Ouyang et al., 2022; Shao et al., 2024;\nTu et al., 2025), achieving significant progress on com-\nplex reasoning tasks. However, the efficacy of these meth-\nods, whether Reinforcement Learning from Human Feed-\nback (RLHF) or Reinforcement Learning from Verifiable\nRewards (RLVR), relies heavily on massive, high-quality,\nhuman-curated datasets (Zhang et al., 2025c). This depen-\ndency not only creates a severe scalability bottleneck (Yue\net al., 2025), which is time-consuming, labor-intensive, and\ncostly, but also fundamentally tethers the potential of AI to\nthe limits of human knowledge and annotation speed.\nTo break free from this reliance on human data, self-\nevolution frameworks have emerged as a promising alterna-\ntive (Zhao et al., 2025; Liu et al., 2025a; Huang et al., 2025;\nWang et al., 2025d), offering a scalable pathway by en-\nabling models to autonomously generate their own training\ndata. Yet, despite their potential, existing self-play or self-\nchallenging approaches face severe constraints. First, their\ncapabilities are capped by the model’s inherent knowledge\nand reasoning abilities (Fang et al., 2025; Cheng et al., 2024;\nZhou et al., 2025a), causing the generated tasks to rarely\nsurpass the model’s current complexity (Zhou et al., 2025b),\nleading to learning stagnation. Second, these frameworks\ntypically operate only in single-round interactions (Li et al.,\n2025c), failing to capture the dynamic, context-dependent\nnature of real-world problems. This dual limitation not only\nrestricts the complexity of the self-generated curriculum but,\nmore critically, hinders the model from mastering essential\nskills that require complex tool use or multi-st\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"file_path\":\"/workplace/papers/llm_agents_tool_learning_survey.pdf\"}",
                        "name": "read_file"
                    },
                    "id": "call_LurV6gyGf0gOm62cIpV1nzqS",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_LurV6gyGf0gOm62cIpV1nzqS",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\nVol.:(0123456789)\nData Science and Engineering\nhttps://doi.org/10.1007/s41019-025-00296-9\nREVIEW/SURVEY PAPERS\nLLM‑Based Agents for Tool Learning: A Survey\nWeikai Xu1 · Chengrui Huang1 · Shen Gao1 · Shuo Shang1\nReceived: 9 October 2024 / Revised: 5 April 2025 / Accepted: 21 April 2025\n© The Author(s) 2025\nAbstract\nHuman beings capable of making and using tools can accomplish tasks far beyond their innate abilities, and this paradigm of\nintegration with tools may not be limited to humans themselves. Recently, the large language model (LLM) has demonstrated\nimmense potential across various fields with its unique planning and reasoning abilities. However, there are still many chal-\nlenges beyond its capabilities due to deficiencies in its training data and inherent illusions. Thus, integrating LLMs and tools\ninto tool learning agents has become a new emerging research direction. To this end, we present a systematic investigation\nand comprehensive review of tool-learning agents in this paper. We start by introducing the definition of the tool learning\ntask for Agents and then illustrating the  typical architecture of the tool-learning models. Since these tools are all defined by\nusers, LLM does not know what tools there are and what their functions are. Thus, LLMs should first find appropriate tools\nand split the tool retrieval methods into two categories: training-based and non-training-based. To accurately complete the\nuser task, it is important to decompose the task into several sub-tasks and execute them in the correct order. Following that,\nwe introduce the tool planning methods and organize these works by whether they rely on the model’s inherent reasoning\ncapabilities for planning or utilize external reasoning tools. Due to the rapid development of this field, we also introduce\nan emerging frontier direction: using multimodal tools for LLM. In addition, we compile current open-source benchmarks\nand evaluation metrics, focusing on their scale, composition, calculation methods, and assessment dimensions. Next, we\nintroduce several application scenarios for the LLM-based tool learning methods. Finally, we discuss the safety and ethical\nissues involved in tool learning.\nKeywords Large language model · Tool learning · Agent · Vision-language model\n1 Introduction\nTool- learning is considered a unique ability of humans [4].\nThe innovation and application of tools expand the range of\navailable resources, empowering humans to overcome physi-\nological limitations and enhancing problem-solving effi-\nciency [37]. Recently, LLMs have demonstrated impressive\nabilities in reasoning and comprehension. There is a growing\ninterest in their potential to interact with the real world and\nenhance their functionality through integration with external\ntools. Inspired by human tool learning, the integration of\ntool-learning capabilities into LLMs has received extensive\nattention in the literature. The goal of tool learning is to\nempower LLMs to understand the functionality of various\ntools and employ them effectively to accomplish specific\ntasks.\nThe integration of multiple inputs, multi-agent planning\nstrategies and reasoning frameworks, along with diverse\ntool repositories, facilitates the development of tool learning\ncapabilities within LLMs. Thus, equipping LLMs with tools\nhas become a new emerging research direction. Currently,\nresearch primarily focuses on two approaches for integrating\nLLMs with tools. The first approach focuses on empowering\nLLMs to master the use and application of tools, thereby\nenhancing LLMs intelligence. The second approach endeav-\nors to view tools as extensions of LLMs inherent capabili-\nties. The two approaches share the same three steps to aug-\nment LLMs with tools: invoking tools at the right time,\nretrieving the appropriate tools, and effectively utilizing\nthem to solve real-world tasks. However, each of the three\nsteps in tool-learning presents several challenges: (1) LLMs\n\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"file_path\":\"/workplace/papers/llm_multi_agent_survey.pdf\"}",
                        "name": "read_file"
                    },
                    "id": "call_vXCkulaFQSpZzL8mY0Wm2egP",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_vXCkulaFQSpZzL8mY0Wm2egP",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\nLi et al. Vicinagearth             (2024) 1:9\nhttps://doi.org/10.1007/s44336-024-00009-2\nREVIEW Open Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line\nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this\nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nVicinagearth\nA survey on LLM-based multi-agent systems:\nworkflow, infrastructure, and challenges\nXinyi Li1, Sai Wang1, Siqi Zeng1, Yu Wu1* and Yi Yang2\nAbstract\nThe pursuit of more intelligent and credible autonomous systems, akin to human society, has been a long-standing\nendeavor for humans. Leveraging the exceptional reasoning and planning capabilities of large language models\n(LLMs), LLM-based agents have been proposed and have achieved remarkable success across a wide array of tasks.\nNotably, LLM-based multi-agent systems (MAS) are considered a promising pathway towards realizing general artifi-\ncial intelligence that is equivalent to or surpasses human-level intelligence. In this paper, we present a comprehensive\nsurvey of these studies, offering a systematic review of LLM-based MAS. Adhering to the workflow of LLM-based\nmulti-agent systems, we synthesize a general structure encompassing five key components: profile, perception, self-\naction, mutual interaction, and evolution. This unified framework encapsulates much of the previous work in the field.\nFurthermore, we illuminate the extensive applications of LLM-based MAS in two principal areas: problem-solving\nand world simulation. Finally, we discuss in detail several contemporary challenges and provide insights into potential\nfuture directions in this domain.\nKeywords Large language model, Multi-agent system, Systematic workflow\n1 Introduction\nEnhancing the reliability and intelligence of autonomous\nintelligent systems has long been regarded as a highly\npromising research avenue. With the advent of the agent\nconcept, which refers to an entity capable of perceiving\nits environment and taking action, agent-based intel -\nligent systems have garnered considerable attention in\nrecent years. Historically, RL-based intelligent systems\nhave dominated this field, wherein agents are typically\nassigned to perform simple, well-defined actions or tasks\nwith constraint interaction with their environment. How-\never, this approach has inherent limitations in terms of\nadaptability and complexity, prompting the exploration\nof more advanced and interactive agent-based systems.\nLarge language models (LLMs) have demonstrated\nexceptional potential in reasoning and planning, aligning\nprecisely with the human expectations for LLM-based\nagents capable of perceiving their surroundings, mak -\ning decisions, and taking actions within an interactive\nenvironment. Motivated by this, LLM-based agents have\nmade significant strides in interacting with complex envi-\nronments and solving intricate tasks across a wide range\nof applications [1], akin to human life in society. Notably,\nLLM-based multi-agent systems have been proposed as\na pivotal pathway to harness collective intelligence while\npreserving the specialized characteristics of individual\nagents, thereby advancing toward more sophisticated\nautonomous intelligent systems. Specifically, multiple\nspecialized agents, endowed with distinct identiti\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"file_path\":\"/workplace/papers/reasoning_action_integration.pdf\"}",
                        "name": "read_file"
                    },
                    "id": "call_hDn9Y1xrzcTiIU0EANnwRjcj",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_hDn9Y1xrzcTiIU0EANnwRjcj",
            "name": "read_file",
            "content": "[The output of the tool `read_file` showing in the interactive terminal]\nTerminal viewport position: Showing page 1 of 1.\n==============================================\nOn Evaluating the Integration of Reasoning and Action in LLM Agents\nwith Database Question Answering\nLinyong Nan1 Ellen Zhang1 Weijin Zou2 Yilun Zhao1\nWenfei Zhou3 Arman Cohan1,4\n1Yale University 2LinkedIn 3NVIDIA Corporation 4Allen Institute for AI\n{linyong.nan, ellen.zhang}@yale.edu\nAbstract\nThis study introduces a new long-form database\nquestion answering dataset designed to evalu-\nate how Large Language Models (LLMs) inter-\nact with a SQL interpreter. The task necessi-\ntates LLMs to strategically generate multiple\nSQL queries to retrieve sufficient data from a\ndatabase, to reason with the acquired context,\nand to synthesize them into a comprehensive\nanalytical narrative. Our findings highlight that\nthis task poses great challenges even for the\nstate-of-the-art GPT-4 model. We propose and\nevaluate two interaction strategies, and provide\na fine-grained analysis of the individual stages\nwithin the interaction. A key discovery is the\nidentification of two primary bottlenecks hin-\ndering effective interaction: the capacity for\nplanning and the ability to generate multiple\nSQL queries. To address the challenge of accu-\nrately assessing answer quality, we introduce\na multi-agent evaluation framework that simu-\nlates the academic peer-review process, enhanc-\ning the precision and reliability of our evalu-\nations. This framework allows for a more nu-\nanced understanding of the strengths and lim-\nitations of current LLMs in complex retrieval\nand reasoning tasks.\n1 Introduction\nSignificant advancements in natural language pro-\ncessing have been driven by the development of\nLarge Language Models (LLMs) (Devlin et al.,\n2019; Radford et al., 2019; Brown et al., 2020;\nChowdhery et al., 2022; OpenAI, 2023), which\nhave become fundamental components of numer-\nous products used by millions, reshaping people’s\nhabits on accessing information. Despite their\nwidespread adoption and impact, LLMs face in-\ntrinsic limitations due to their design, including\nlimited context window, stochastic nature which\nmakes them less suited for tasks requiring high\nstandards of precision, and extensive computations\n(Mialon et al., 2023; Ji et al., 2023; Wang et al.,\n2023a). Many studies have explored ways to miti-\ngate these constraints by augmenting LLMs with\nmodules/tools of complementary features (Nakano\net al., 2022; Lewis et al., 2020; Lazaridou et al.,\n2022; Gao et al., 2023a; Parisi et al., 2022; Schick\net al., 2023). In our study, we focus on augment-\ning LLMs with a symbolic module - a SQL code\ninterpreter - and assess their performance using the\nlong-form database question-answering task that\nwe introduce, illustrated in Figure 1. Such augmen-\ntation is inevitable for tasks involving databases,\nas they often far exceed the size of LLMs’ context\nwindows1, making information retrieval through\nany means other than SQL inefficient. Addition-\nally, the use of SQL queries brings transparency to\nthe reasoning process of LLM agents, providing a\nmeans to validate the accuracy of their generated\nresponses.\nLLMs augmented with external modules/tools\npossess two primary abilities: the capacity to act,\nwhich involves the use of tools, and the capabil-\nity to reason, which encompasses planning and\nanalyzing the outcomes of actions (Mialon et al.,\n2023; Madaan et al., 2023; Paul et al., 2023; Yao\net al., 2023; Yoran et al., 2023; Shinn et al., 2023).\nWhile numerous studies have evaluated these abili-\nties in different contexts, we contend that some of\nthem focus more on evaluating tool selection and\ntool employment with less focus on evaluating how\nLLM agents reflect or synthesize the action results\n(Parisi et al., 2022; Schick et al., 2023; Zhuang\net al., 2023; Li et al., 2023). Other research (Shus-\nter et al., 2022; Yao et al., 2023; BehnamGhader\net al., 2023) does examine both the action and rea-\nsoning capacities of LLM agents, yet the actions’\ncomplexity is not as demanding as in studies with\na stronger focus on the action aspect. Our goals\nare twofold: firstly\n\n==============================================\n"
        },
        {
            "content": "",
            "role": "assistant",
            "tool_calls": [
                {
                    "index": 0,
                    "function": {
                        "arguments": "{\"reference_codebases\":[\"Agent0\",\"LLM-Based Toot Learning Survey\",\"LLM-Based Multi-Agent Systems\",\"Reasoning + Action Integration\"],\"reference_paths\":[\"https://github.com/aiming-lab/Agent0\",\"https://doi.org/10.1007/s41019-025-00296-9\",\"https://doi.org/10.1007/s44336-024-00009-2\",\"https://doi.org/10.1007/s42247-025-00007-4\"],\"reference_papers\":[\"Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning\",\"LLM-Based Agents for Tool Learning: A Survey\",\"A Survey on LLM-based Multi-Agent Systems: Workflow, Infrastructure, and Challenges\",\"On Evaluating the Integration of Reasoning and Action in LLM Agents\"]}",
                        "name": "case_resolved"
                    },
                    "id": "call_8wBAKE3QF14agELw7YY3AinC",
                    "type": "function"
                }
            ],
            "function_call": null,
            "sender": "Prepare Agent"
        },
        {
            "role": "tool",
            "tool_call_id": "call_8wBAKE3QF14agELw7YY3AinC",
            "name": "case_resolved",
            "content": "I have determined the reference codebases and paths according to the existing resources and the innovative ideas.\n{\n    \"reference_codebases\": [\n        \"Agent0\",\n        \"LLM-Based Toot Learning Survey\",\n        \"LLM-Based Multi-Agent Systems\",\n        \"Reasoning + Action Integration\"\n    ],\n    \"reference_paths\": [\n        \"https://github.com/aiming-lab/Agent0\",\n        \"https://doi.org/10.1007/s41019-025-00296-9\",\n        \"https://doi.org/10.1007/s44336-024-00009-2\",\n        \"https://doi.org/10.1007/s42247-025-00007-4\"\n    ],\n    \"reference_papers\": [\n        \"Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning\",\n        \"LLM-Based Agents for Tool Learning: A Survey\",\n        \"A Survey on LLM-based Multi-Agent Systems: Workflow, Infrastructure, and Challenges\",\n        \"On Evaluating the Integration of Reasoning and Action in LLM Agents\"\n    ]\n}\n"
        }
    ],
    "context_variables": {
        "working_dir": "workplace",
        "date_limit": "2025-11-20",
        "prepare_result": {
            "reference_codebases": [
                "Agent0",
                "LLM-Based Toot Learning Survey",
                "LLM-Based Multi-Agent Systems",
                "Reasoning + Action Integration"
            ],
            "reference_paths": [
                "https://github.com/aiming-lab/Agent0",
                "https://doi.org/10.1007/s41019-025-00296-9",
                "https://doi.org/10.1007/s44336-024-00009-2",
                "https://doi.org/10.1007/s42247-025-00007-4"
            ],
            "reference_papers": [
                "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
                "LLM-Based Agents for Tool Learning: A Survey",
                "A Survey on LLM-based Multi-Agent Systems: Workflow, Infrastructure, and Challenges",
                "On Evaluating the Integration of Reasoning and Action in LLM Agents"
            ]
        }
    }
}